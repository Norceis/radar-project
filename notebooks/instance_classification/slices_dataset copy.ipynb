{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys \n",
    "sys.path.append('../../')\n",
    "\n",
    "from source.helper import to_dB, print_spectogram, gen_spectogram, diff_frames, \\\n",
    "                          get_argmaxed_spectrogram, get_tresholded_spectogram,  \\\n",
    "                          get_spectrogram_metrics, plot_metrics, get_spectogram_slices\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class SampleType(Enum):\n",
    "    BARTEK = 0\n",
    "    KUBA = 1\n",
    "    OSKAR = 2\n",
    "    RAFAL = 3\n",
    "    FAKE = 4\n",
    "    NOISE = 5\n",
    "    AUTO = 6\n",
    "    WIATRAK = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"D:/SIIUM/3 semestr/RADAR/radar-project/data\")  # change to your project dir\n",
    "\n",
    "loaded_file = h5py.File(BASE_DIR / 'complete_dataset.h5', 'r') # best to make yourself a h5 file containing complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['background',\n",
       " 'bartek',\n",
       " 'kuba',\n",
       " 'oddychacz_2m_o0_s100',\n",
       " 'oddychacz_2m_o30_s0',\n",
       " 'oddychacz_2m_o30_s100',\n",
       " 'oddychacz_3m_o0_s100',\n",
       " 'oddychacz_3m_o30_s0',\n",
       " 'oddychacz_3m_o30_s100',\n",
       " 'oddychacz_4m_o0_s100',\n",
       " 'oddychacz_4m_o30_s0',\n",
       " 'oddychacz_4m_o30_s100',\n",
       " 'oskar',\n",
       " 'rafal',\n",
       " 'randomowe_chodzonko_bartek',\n",
       " 'randomowe_chodzonko_kuba',\n",
       " 'randomowe_chodzonko_oskar',\n",
       " 'randomowe_chodzonko_rafal',\n",
       " 'samochod',\n",
       " 'wiatrak_2m',\n",
       " 'wiatrak_3m',\n",
       " 'wiatrak_4m',\n",
       " 'wiatrak_foliarz_2m',\n",
       " 'wiatrak_foliarz_3m',\n",
       " 'wiatrak_foliarz_4m']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(loaded_file['2023_05_08'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = list(loaded_file['2023_05_08'].keys())\n",
    "\n",
    "dict_classes = {\n",
    "        'background': SampleType.NOISE,\n",
    "        'bartek': SampleType.BARTEK,\n",
    "        'kuba': SampleType.KUBA,\n",
    "        'oddychacz_2m_o0_s100': SampleType.FAKE,\n",
    "        'oddychacz_2m_o30_s0': SampleType.FAKE,\n",
    "        'oddychacz_2m_o30_s100': SampleType.FAKE,\n",
    "        'oddychacz_3m_o0_s100': SampleType.FAKE,\n",
    "        'oddychacz_3m_o30_s0': SampleType.FAKE,\n",
    "        'oddychacz_3m_o30_s100': SampleType.FAKE,\n",
    "        'oddychacz_4m_o0_s100': SampleType.FAKE,\n",
    "        'oddychacz_4m_o30_s0': SampleType.FAKE,\n",
    "        'oddychacz_4m_o30_s100': SampleType.FAKE,\n",
    "        'oskar': SampleType.OSKAR,\n",
    "        'rafal': SampleType.RAFAL,\n",
    "        'randomowe_chodzonko_bartek': SampleType.BARTEK,\n",
    "        'randomowe_chodzonko_kuba': SampleType.KUBA,\n",
    "        'randomowe_chodzonko_oskar': SampleType.OSKAR,\n",
    "        'randomowe_chodzonko_rafal': SampleType.RAFAL,\n",
    "        'samochod': SampleType.AUTO,\n",
    "        'wiatrak_2m': SampleType.WIATRAK,\n",
    "        'wiatrak_3m': SampleType.WIATRAK,\n",
    "        'wiatrak_4m': SampleType.WIATRAK,\n",
    "        'wiatrak_foliarz_2m': SampleType.WIATRAK,\n",
    "        'wiatrak_foliarz_3m': SampleType.WIATRAK,\n",
    "        'wiatrak_foliarz_4m': SampleType.WIATRAK,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not exist, creating file\n"
     ]
    }
   ],
   "source": [
    "H5_FILENAME = 'slices_dataset'\n",
    "\n",
    "try:\n",
    "    h5file = h5py.File(H5_FILENAME + '.h5','r+')\n",
    "except:\n",
    "    print('File not exist, creating file')\n",
    "    h5file = h5py.File(H5_FILENAME + '.h5','w')\n",
    "\n",
    "samples_ds =  h5file.create_dataset(name='samples',\n",
    "                                shape=(0, 64, 128, 8), # dim 0 so without first zero-initialized sample\n",
    "                                maxshape=(None, 64, 128, 8))\n",
    "labels_ds =  h5file.create_dataset(name='labels',\n",
    "                                shape=(0,8),\n",
    "                                maxshape=(None,8)\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background SampleType.NOISE\n",
      "(13060, 512)\n",
      "(64, 12960, 8)\n",
      "(201, 64, 128, 8)\n",
      "(201, 8)\n",
      "dataset size after appending SampleType.NOISE: 201\n",
      "bartek SampleType.BARTEK\n",
      "(86988, 512)\n",
      "(64, 86888, 8)\n",
      "(1356, 64, 128, 8)\n",
      "(1356, 8)\n",
      "dataset size after appending SampleType.BARTEK: 1557\n",
      "kuba SampleType.KUBA\n",
      "(86273, 512)\n",
      "(64, 86173, 8)\n",
      "(1345, 64, 128, 8)\n",
      "(1345, 8)\n",
      "dataset size after appending SampleType.KUBA: 2902\n",
      "oddychacz_2m_o0_s100 SampleType.FAKE\n",
      "(15655, 512)\n",
      "(64, 15555, 8)\n",
      "(242, 64, 128, 8)\n",
      "(242, 8)\n",
      "dataset size after appending SampleType.FAKE: 3144\n",
      "oddychacz_2m_o30_s0 SampleType.FAKE\n",
      "(14697, 512)\n",
      "(64, 14597, 8)\n",
      "(227, 64, 128, 8)\n",
      "(227, 8)\n",
      "dataset size after appending SampleType.FAKE: 3371\n",
      "oddychacz_2m_o30_s100 SampleType.FAKE\n",
      "(18259, 512)\n",
      "(64, 18159, 8)\n",
      "(282, 64, 128, 8)\n",
      "(282, 8)\n",
      "dataset size after appending SampleType.FAKE: 3653\n",
      "oddychacz_3m_o0_s100 SampleType.FAKE\n",
      "(14870, 512)\n",
      "(64, 14770, 8)\n",
      "(229, 64, 128, 8)\n",
      "(229, 8)\n",
      "dataset size after appending SampleType.FAKE: 3882\n",
      "oddychacz_3m_o30_s0 SampleType.FAKE\n",
      "(15269, 512)\n",
      "(64, 15169, 8)\n",
      "(236, 64, 128, 8)\n",
      "(236, 8)\n",
      "dataset size after appending SampleType.FAKE: 4118\n",
      "oddychacz_3m_o30_s100 SampleType.FAKE\n",
      "(13943, 512)\n",
      "(64, 13843, 8)\n",
      "(215, 64, 128, 8)\n",
      "(215, 8)\n",
      "dataset size after appending SampleType.FAKE: 4333\n",
      "oddychacz_4m_o0_s100 SampleType.FAKE\n",
      "(15772, 512)\n",
      "(64, 15672, 8)\n",
      "(243, 64, 128, 8)\n",
      "(243, 8)\n",
      "dataset size after appending SampleType.FAKE: 4576\n",
      "oddychacz_4m_o30_s0 SampleType.FAKE\n",
      "(21374, 512)\n",
      "(64, 21274, 8)\n",
      "(331, 64, 128, 8)\n",
      "(331, 8)\n",
      "dataset size after appending SampleType.FAKE: 4907\n",
      "oddychacz_4m_o30_s100 SampleType.FAKE\n",
      "(20968, 512)\n",
      "(64, 20868, 8)\n",
      "(325, 64, 128, 8)\n",
      "(325, 8)\n",
      "dataset size after appending SampleType.FAKE: 5232\n",
      "oskar SampleType.OSKAR\n",
      "(98891, 512)\n",
      "(64, 98791, 8)\n",
      "(1542, 64, 128, 8)\n",
      "(1542, 8)\n",
      "dataset size after appending SampleType.OSKAR: 6774\n",
      "rafal SampleType.RAFAL\n",
      "(90091, 512)\n",
      "(64, 89991, 8)\n",
      "(1405, 64, 128, 8)\n",
      "(1405, 8)\n",
      "dataset size after appending SampleType.RAFAL: 8179\n",
      "randomowe_chodzonko_bartek SampleType.BARTEK\n",
      "(56446, 512)\n",
      "(64, 56346, 8)\n",
      "(879, 64, 128, 8)\n",
      "(879, 8)\n",
      "dataset size after appending SampleType.BARTEK: 9058\n",
      "randomowe_chodzonko_kuba SampleType.KUBA\n",
      "(61301, 512)\n",
      "(64, 61201, 8)\n",
      "(955, 64, 128, 8)\n",
      "(955, 8)\n",
      "dataset size after appending SampleType.KUBA: 10013\n",
      "randomowe_chodzonko_oskar SampleType.OSKAR\n",
      "(60749, 512)\n",
      "(64, 60649, 8)\n",
      "(946, 64, 128, 8)\n",
      "(946, 8)\n",
      "dataset size after appending SampleType.OSKAR: 10959\n",
      "randomowe_chodzonko_rafal SampleType.RAFAL\n",
      "(59351, 512)\n",
      "(64, 59251, 8)\n",
      "(924, 64, 128, 8)\n",
      "(924, 8)\n",
      "dataset size after appending SampleType.RAFAL: 11883\n",
      "samochod SampleType.AUTO\n",
      "(27354, 512)\n",
      "(64, 27254, 8)\n",
      "(424, 64, 128, 8)\n",
      "(424, 8)\n",
      "dataset size after appending SampleType.AUTO: 12307\n",
      "wiatrak_2m SampleType.WIATRAK\n",
      "(16950, 512)\n",
      "(64, 16850, 8)\n",
      "(262, 64, 128, 8)\n",
      "(262, 8)\n",
      "dataset size after appending SampleType.WIATRAK: 12569\n",
      "wiatrak_3m SampleType.WIATRAK\n",
      "(14652, 512)\n",
      "(64, 14552, 8)\n",
      "(226, 64, 128, 8)\n",
      "(226, 8)\n",
      "dataset size after appending SampleType.WIATRAK: 12795\n",
      "wiatrak_4m SampleType.WIATRAK\n",
      "(15610, 512)\n",
      "(64, 15510, 8)\n",
      "(241, 64, 128, 8)\n",
      "(241, 8)\n",
      "dataset size after appending SampleType.WIATRAK: 13036\n",
      "wiatrak_foliarz_2m SampleType.WIATRAK\n",
      "(15360, 512)\n",
      "(64, 15260, 8)\n",
      "(237, 64, 128, 8)\n",
      "(237, 8)\n",
      "dataset size after appending SampleType.WIATRAK: 13273\n",
      "wiatrak_foliarz_3m SampleType.WIATRAK\n",
      "(14948, 512)\n",
      "(64, 14848, 8)\n",
      "(231, 64, 128, 8)\n",
      "(231, 8)\n",
      "dataset size after appending SampleType.WIATRAK: 13504\n",
      "wiatrak_foliarz_4m SampleType.WIATRAK\n",
      "(21442, 512)\n",
      "(64, 21342, 8)\n",
      "(332, 64, 128, 8)\n",
      "(332, 8)\n",
      "dataset size after appending SampleType.WIATRAK: 13836\n"
     ]
    }
   ],
   "source": [
    "margin = 500\n",
    "\n",
    "for key in data_keys:\n",
    "\n",
    "    print(key, dict_classes[key])\n",
    "\n",
    "    sample_file = loaded_file['2023_05_08'][key]\n",
    "    sample_file = sample_file[margin:-margin, 0, 0, :]\n",
    "    print(sample_file.shape)\n",
    "    slices_diff = []\n",
    "    for d in [1,2,3,5,10,25,50,100]:\n",
    "        frames_diff = diff_frames(sample_file, d)  #odejmujemy ostatnią klatkę\n",
    "        diff_spect, y = gen_spectogram(frames_diff)\n",
    "        diff_spect = diff_spect[:64] # 64 is max height of spectrogram\n",
    "\n",
    "        diff_spectdb = to_dB(diff_spect)\n",
    "        argmax_spectrogram = get_argmaxed_spectrogram(diff_spectdb)[:,100-d:] # this returns spectrogram with 1 at argmax, else 0\n",
    "        slices_diff.append(argmax_spectrogram)\n",
    "    # stack slices on new axis\n",
    "    slices_diff = np.stack(slices_diff, axis=2)\n",
    "\n",
    "    print(slices_diff.shape)\n",
    "    class_id = dict_classes[key].value\n",
    "    slices = get_spectogram_slices(slices_diff)\n",
    "    print(slices.shape)\n",
    "    labels = np.full(slices.shape[0], class_id)\n",
    "    labels = to_categorical(labels, num_classes=8)\n",
    "    print(labels.shape)\n",
    "\n",
    "    # append to dataset\n",
    "    index = len(samples_ds)\n",
    "    add_size = slices.shape[0]\n",
    "    samples_ds.resize((index + add_size, 64, 128, 8))\n",
    "    labels_ds.resize((index + add_size, 8))\n",
    "    samples_ds[-add_size:] = slices\n",
    "    labels_ds[-add_size:] = labels\n",
    "    \n",
    "    print(f'dataset size after appending {dict_classes[key]}: {samples_ds.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not exist, creating file\n",
      "rafal SampleType.RAFAL\n",
      "(90091, 512)\n",
      "(64, 89991, 8)\n",
      "(1405, 64, 128, 8)\n",
      "randomowe_chodzonko_rafal SampleType.RAFAL\n",
      "(59351, 512)\n",
      "(64, 59251, 8)\n",
      "(924, 64, 128, 8)\n",
      "background SampleType.NOISE\n",
      "(13060, 512)\n",
      "(64, 12960, 8)\n",
      "(201, 64, 128, 8)\n",
      "oskar SampleType.OSKAR\n",
      "(98891, 512)\n",
      "(64, 98791, 8)\n",
      "(1542, 64, 128, 8)\n",
      "randomowe_chodzonko_oskar SampleType.OSKAR\n",
      "(60749, 512)\n",
      "(64, 60649, 8)\n",
      "(946, 64, 128, 8)\n",
      "oddychacz_2m_o0_s100 SampleType.FAKE\n",
      "(15655, 512)\n",
      "(64, 15555, 8)\n",
      "(242, 64, 128, 8)\n",
      "oddychacz_2m_o30_s0 SampleType.FAKE\n",
      "(14697, 512)\n",
      "(64, 14597, 8)\n",
      "(227, 64, 128, 8)\n",
      "oddychacz_2m_o30_s100 SampleType.FAKE\n",
      "(18259, 512)\n",
      "(64, 18159, 8)\n",
      "(282, 64, 128, 8)\n",
      "oddychacz_3m_o0_s100 SampleType.FAKE\n",
      "(14870, 512)\n",
      "(64, 14770, 8)\n",
      "(229, 64, 128, 8)\n",
      "oddychacz_3m_o30_s0 SampleType.FAKE\n",
      "(15269, 512)\n",
      "(64, 15169, 8)\n",
      "(236, 64, 128, 8)\n",
      "oddychacz_3m_o30_s100 SampleType.FAKE\n",
      "(13943, 512)\n",
      "(64, 13843, 8)\n",
      "(215, 64, 128, 8)\n",
      "oddychacz_4m_o0_s100 SampleType.FAKE\n",
      "(15772, 512)\n",
      "(64, 15672, 8)\n",
      "(243, 64, 128, 8)\n",
      "oddychacz_4m_o30_s0 SampleType.FAKE\n",
      "(21374, 512)\n",
      "(64, 21274, 8)\n",
      "(331, 64, 128, 8)\n",
      "oddychacz_4m_o30_s100 SampleType.FAKE\n",
      "(20968, 512)\n",
      "(64, 20868, 8)\n",
      "(325, 64, 128, 8)\n",
      "wiatrak_2m SampleType.WIATRAK\n",
      "(16950, 512)\n",
      "(64, 16850, 8)\n",
      "(262, 64, 128, 8)\n",
      "wiatrak_3m SampleType.WIATRAK\n",
      "(14652, 512)\n",
      "(64, 14552, 8)\n",
      "(226, 64, 128, 8)\n",
      "wiatrak_4m SampleType.WIATRAK\n",
      "(15610, 512)\n",
      "(64, 15510, 8)\n",
      "(241, 64, 128, 8)\n",
      "wiatrak_foliarz_2m SampleType.WIATRAK\n",
      "(15360, 512)\n",
      "(64, 15260, 8)\n",
      "(237, 64, 128, 8)\n",
      "wiatrak_foliarz_3m SampleType.WIATRAK\n",
      "(14948, 512)\n",
      "(64, 14848, 8)\n",
      "(231, 64, 128, 8)\n",
      "wiatrak_foliarz_4m SampleType.WIATRAK\n",
      "(21442, 512)\n",
      "(64, 21342, 8)\n",
      "(332, 64, 128, 8)\n",
      "bartek SampleType.BARTEK\n",
      "(86988, 512)\n",
      "(64, 86888, 8)\n",
      "(1356, 64, 128, 8)\n",
      "randomowe_chodzonko_bartek SampleType.BARTEK\n",
      "(56446, 512)\n",
      "(64, 56346, 8)\n",
      "(879, 64, 128, 8)\n",
      "kuba SampleType.KUBA\n",
      "(86273, 512)\n",
      "(64, 86173, 8)\n",
      "(1345, 64, 128, 8)\n",
      "randomowe_chodzonko_kuba SampleType.KUBA\n",
      "(61301, 512)\n",
      "(64, 61201, 8)\n",
      "(955, 64, 128, 8)\n",
      "samochod SampleType.AUTO\n",
      "(27354, 512)\n",
      "(64, 27254, 8)\n",
      "(424, 64, 128, 8)\n"
     ]
    }
   ],
   "source": [
    "margin = 500\n",
    "\n",
    "H5_FILENAME = 'slices_dataset'\n",
    "\n",
    "try:\n",
    "    h5file = h5py.File(H5_FILENAME + '.h5','r+')\n",
    "except:\n",
    "    print('File not exist, creating file')\n",
    "    h5file = h5py.File(H5_FILENAME + '.h5','w')\n",
    "\n",
    "for sample_type in set(dict_classes.values()):\n",
    "    slices_list = []\n",
    "    for key in [key for key in data_keys if dict_classes[key] == sample_type]:\n",
    "        print(key, dict_classes[key])\n",
    "\n",
    "        sample_file = loaded_file['2023_05_08'][key]\n",
    "        sample_file = sample_file[margin:-margin, 0, 0, :]\n",
    "        print(sample_file.shape)\n",
    "        slices_diff = []\n",
    "        for d in [1,2,3,5,10,25,50,100]:\n",
    "            frames_diff = diff_frames(sample_file, d)  #odejmujemy ostatnią klatkę\n",
    "            diff_spect, y = gen_spectogram(frames_diff)\n",
    "            diff_spect = diff_spect[:64]\n",
    "\n",
    "            diff_spectdb = to_dB(diff_spect)\n",
    "            argmax_spectrogram = get_argmaxed_spectrogram(diff_spectdb)[:,100-d:] # this returns spectrogram with 1 at argmax, else 0\n",
    "            slices_diff.append(argmax_spectrogram)\n",
    "        # stack slices on new axis\n",
    "        slices_diff = np.stack(slices_diff, axis=2)\n",
    "\n",
    "        print(slices_diff.shape)\n",
    "        slices = get_spectogram_slices(slices_diff, window_size=128)\n",
    "        print(slices.shape) \n",
    "        slices_list.append(slices)\n",
    "\n",
    "    slices = np.concatenate(slices_list, axis=0)\n",
    "    h5file.create_dataset(sample_type.name, data = slices)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_h5 = h5py.File('slices_dataset.h5', 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = new_h5['AUTO']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
